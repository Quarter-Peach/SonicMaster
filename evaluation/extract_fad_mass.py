import argparse
import os
import json
from pathlib import Path

import numpy as np
import pandas as pd
from tqdm import tqdm
from laion_clap import CLAP_Module
from scipy.linalg import sqrtm

def load_clean_embeddings(npz_path):
    data = np.load(npz_path, allow_pickle=True)
    emb = data["embeddings"]
    fnames = data["filenames"]
    fname_to_emb = {fname: emb[i] for i, fname in enumerate(fnames)}
    return fname_to_emb

# def load_test_entries(jsonl_path):
#     entries = []
#     with open(jsonl_path, "r") as f:
#         for line in f:
#             entry = json.loads(line)
#             entries.append((entry["id"], entry["original_id"]))
#     return entries


def load_test_entries(jsonl_path):
    entries = []
    with open(jsonl_path, "r") as f:
        for line in f:
            entry = json.loads(line)
            degradations = entry.get("degradations", [])
            # if len(degradations) > 1:
            #     continue
            # if len(degradations) < 2:
            #     continue
            original_id = entry.get("original_id", entry["id"])
            entries.append((entry["id"], original_id))
    return entries


def extract_degraded_embeddings(model, entries, degraded_folder, clean_lookup):
    degraded_embeddings = []
    clean_embeddings = []

    for degraded_id, original_id in tqdm(entries, desc=f"Extracting from {os.path.basename(degraded_folder)}"):
        degraded_path = os.path.join(degraded_folder, f"{degraded_id}.flac")
        if not os.path.exists(degraded_path):
            print(f"âš ï¸  Missing degraded file: {degraded_path}")
            continue
        if original_id not in clean_lookup:
            print(f"âš ï¸  Clean embedding for '{original_id}' not found.")
            continue
        try:
            emb = model.get_audio_embedding_from_filelist([degraded_path], use_tensor=False)
            if len(emb) == 0:
                print(f"âŒ No embedding for {degraded_path}")
                continue
            degraded_embeddings.append(emb[0])
            clean_embeddings.append(clean_lookup[original_id])
        except Exception as e:
            print(f"âŒ Failed on {degraded_path}: {e}")
            continue

    return np.stack(clean_embeddings), np.stack(degraded_embeddings)

def frechet_distance(mu1, sigma1, mu2, sigma2):
    diff = mu1 - mu2
    covmean = sqrtm(sigma1 @ sigma2)
    if np.iscomplexobj(covmean):
        covmean = covmean.real
    return diff @ diff + np.trace(sigma1 + sigma2 - 2 * covmean)

def compute_fad(clean_embs, degraded_embs):
    mu1 = np.mean(clean_embs, axis=0)
    mu2 = np.mean(degraded_embs, axis=0)
    sigma1 = np.cov(clean_embs, rowvar=False)
    sigma2 = np.cov(degraded_embs, rowvar=False)
    return frechet_distance(mu1, sigma1, mu2, sigma2)



def prepare_clap_model(clap_ckpt=None):
    model = CLAP_Module(enable_fusion=False)
    try:
        if clap_ckpt:
            model.load_ckpt(ckpt=clap_ckpt)
        else:
            model.load_ckpt()
    except KeyError as e:
        if 'text_branch.embeddings.position_ids' in str(e):
            print("Handling KeyError in CLAP checkpoint loading...")
            import torch

            state_dict = torch.load(clap_ckpt, map_location='cpu')
            if 'text_branch.embeddings.position_ids' in state_dict:
                del state_dict['text_branch.embeddings.position_ids']
            model.load_state_dict(state_dict, strict=False)
        else:
            raise
    model.eval()
    return model


def run_fad(jsonl_file, folders, clean_npz, output_csv=None, model=None, clap_ckpt=None):
    clean_lookup = load_clean_embeddings(clean_npz)
    entries = load_test_entries(jsonl_file)
    if model is None:
        model = prepare_clap_model(clap_ckpt)

    results = []
    for folder in folders:
        print(f"\nðŸš€ Processing: {folder}")
        clean_embs, degraded_embs = extract_degraded_embeddings(model, entries, folder, clean_lookup)

        if len(clean_embs) == 0 or len(degraded_embs) == 0:
            print(f"âŒ Skipping {folder} â€” no valid embeddings")
            continue

        print(f"ðŸ“Š Computing FAD for {len(clean_embs)} pairs...")
        fad_value = compute_fad(clean_embs, degraded_embs)

        results.append({
            "folder": os.path.basename(folder),
            "num_pairs": len(clean_embs),
            "fad": fad_value
        })

    if output_csv and results:
        Path(output_csv).parent.mkdir(parents=True, exist_ok=True)
        pd.DataFrame(results).to_csv(output_csv, index=False)
        print(f"\nâœ… All FAD results saved: {output_csv}")
        print(pd.DataFrame(results))

    return results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Compute FAD for inference folders")
    parser.add_argument("--jsonl-path", default="/testset_pt.jsonl")
    parser.add_argument("--clean-embeddings", default="/inspire/hdd/global_user/chenxie-25019/HaoQiu/RESULT/clean_embeddings.npz")
    parser.add_argument(
        "--folders",
        nargs="+",
        default=["outputs/run1", "outputs/run2"],
        help="Paths to inference folders"
    )
    parser.add_argument("--output-csv", default="/evaluationfinal/fad_results_all.csv")
    parser.add_argument("--clap-ckpt", help="Local CLAP checkpoint to avoid downloads")
    args = parser.parse_args()
    run_fad(
        args.jsonl_path,
        args.folders,
        args.clean_embeddings,
        args.output_csv,
        clap_ckpt=args.clap_ckpt,
    )
